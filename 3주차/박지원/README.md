# 3장 빅데이터 분산 처리

## 3-1 대규모 분산 처리의 프레임워크

스키마: 테이블의 칼럼명, 데이터형, 테이블 간의 관계

빅데이터의 경우 구조화 된 데이터만 있는 것이 아니라, 자연 언어로 작성된 택스트 데이터와 이미지, 동영상 등의 미디어 데이터도 포함된다. 이러한 스키마가 없는 데이터를 “비구조화 데이터”라고 하고, 이 상태로는 SQL로 제대로 집계할 수 없다.

<img src="assets/image.png" width="400"/>

비구조화 데이터를 분산 스토리지에 저장하여 분산 시스템에서 처리하는 것이 데이터 레이크이다.

### 데이터 구조화의 파이프라인

각 데이터 소스에서 수집된 비구조화 데이터, 또는 스키마리스 데이터는 처음에는 분산 스토리지에 보관된다. 분산 스토리지에 수집된 데이터는 면확한 스키마를 갖지 않는 것도 많으므로 그냥 그대로는 SQL로 집계할 수 없다. 따라서 스키마를 명확하게 한 테이블 형식의 구조화 데이터로 변환하는 과정을 먼저 필요로 한다.

<img src="assets/image 1.png" width="400"/>

구조화 데이터는 데이터의 압축률을 높이기 위해 열 지향 스토리지로 저장한다. MPP 데이터베이스로 전송하거나 Hadoop 상에서 열 지향 스토리지 형식으로 변환한다. 

구조화 데이터 중 시간에 따라 증가하는 데이터를 팩트 테이블, 디멘전 테이블로 취급한다. 이 단계에서는 테이블을 조인하지 않는다. 데이터 마트에 대해 생각하는 것은 좀 더 나중의 이야기다. 여기에는 먼저 데이터를 구조화하여 SQL로 집계 가능한 테이블을 만든는 것부터 생각한다

### 열 지향 스토리지의 작성

MPP 데이터베이스의 경우, 제품에 따라 스토리지의 형식이 고정되어 있어 사용자가 그상세를 몰라도 괜찮지만, Haoop에서는 사용자가 직접 열 지향 스토리지의 형식을 선택하고, 자신의 좋아하는 쿼리 엔진에서 그것을 집계할 수 있다.

Hadoop에서 사용할 수 잇는 열 지향 스토리지에는 몇 가지 종류가 있으며, 각각 특징이 다르다. Apache ORC는 구조화 데이터를 위한 열 지향 스토리지로 처음에 스키마를 정한 후 데이터를 저장한다. 한편 Apache Parquet은 스키마리스에 가까운 데이터 구조로 되어 있어 JSON 같은 뒤얽힌 데이터도 그래도 저장할 수 있다.

비구조화 데이터를 읽어 들여 열 지향 스토리지로 변환하는 과정에서 데이터의 가공 및 압축을 위해 많은 컴퓨터 리소스가 소비된다. 이러한 과정을 위해 Hadoop, Spark와 같은 분산 처리 프레임워크를 사용한다.

### Hadoop

Hadoop은 단일 소프트웨어가 아니라 분산 시스템을 구성하는 다수의 소프트웨어로 이루어진 집합체다. 2013년에 배포된 Hadoop2부터 YARN이라고 불리는 새로운 리소스 관리자 상에 복수의 분산 애플리케이션이 동작하는 구성으로 되어, 대규모 분산시스템을 구축하기 위한 공통 플랫폼의 역활을 담당하고 있다.

<img src="assets/image 2.png" width="400"/>

Hadoop의 기본 구성 요소는 분산 파일 시스템(HDFS), 리소스 관리자(Resource Manager), 분산 데이터 처리(MapReduce) 3가지 이다. 그 외의 프로젝트는 Hadoop 본체와는 독립적으로 개발되어 Hadoop을 이용한 분산 애플리케이션으로 동작한다.

모든 분산 시스템이 Hadoop에 의존하는 것이 아니라, Hadoop을 일부만 사용하거나 혹은 전혀 이용하지 않는 구성도 있다. 예를 들어, 분산 파일 시스템으로는 HDFS를 사용하면서 리소스 관리자는 Mesos 분산 데이터 처리는 Spark를 사용하도록 구성할 수 있다. 이와 같이 다양한 소스트웨어 중에서 자신에게 맞는 것을 선택하고 그것을 조합함으로써 시스템을 구성하는 것이 Hadoop을 중심으로 하는 데이터 처리의 특징이다.

### 분산 파일 시스템과 리소스 관리자

Hadoop에서 처리되는 데이터 대부분은 분산 파일 시스템인 HDFS에 저장한다. HDFS는 다수의 컴퓨터에 파일을 복사하여 중복성을 높인다는 특징이 있다.

CPU나 메모리 등의 계산 리소스는 리소스 매니저인 YARN에 의해 관리된다. YARN은 애플리케이션이 사용하는 CPU 코어와 메모리를 컨테이너라 불리는 단위로 관리한다. Hadoop에서는 분산 애플리케이션을 실행하면 YARN이 클러스터 전체의 부하를 보고 비어 있는 호스트로부터 컨테이너를 할당한다.

<img src="assets/image 2-1.png" width="400"/>

분산 시스템은 많은 계산 리소스를 소비하지만, 호스트의 수에 따라 사용할 수 있는 리소스의 상한이 결정된다. 한정된 리소스로 다수의 분산 애플리케이션이 동시에 실행되므로 애플리케이션 간에 리소스 쟁탈이 발생하고, 이때 리소스 관리자가 이를 관리하여 예방한다.

### 분산 데이터 처리 및 쿼리 엔진

MapReduce도 YARN 상에서 동작하는 분산 애플리케이션 중 하나이며 분산 시스템에서 데이터 처리를 실행하는 데사용된다. MapReduce는 임의의 자바 프로그램을 실행시킬 수 있기 때문에 비구조화 데이터를 가공하는데 적합하다.

SQL등의 쿼리 언어에 의한 데이터 집계가 목적이라면 그것을 위해 설계된 쿼리 엔진을 사용한다. Apache Hive는 그런 쿼리 엔진 중 하나이며, 쿼리를 자동으로 MapReduce 프로그램으로 변환하는 소프트웨어로 개발되었다. 초기 Hive 실행 특성은 MapReduce에 의존하였기도 했다.

MapReduce는 원래 대량의 데이터를 배치 처리하기 위한 시스템이다. 한 번 실행하면 분산 파일 시스템에서 대량의 데이터를 읽을 수 있지만, 작은 프로그램을 실행하려면 오버헤드가 크기 때문에 짧은 쿼리 실행에는 적합하지 않다. Hive도 동일하게 애드 혹 쿼리에는 부적합한 반면 시간이 걸리는 배치 처리에는 적합하다.

<img src="assets/image 3.png" width="400"/>

### 대화형 쿼리 엔진

Hive를 고속화하는 것이 아니라 처음부터 대화형의 쿼리 실행만 전문으로 하는 Apache Impala, Presto와 같은 쿼리 엔진도 개발되고 있다.

MapReduce와 Tez는 장시간의 배치 처리를 가정해 한정된 리소스를 유효하게 활용하도록 설계되어 있다. 한편 대화형 쿼리 엔진으로는 순간 최대 속도를 높이기 위해 모든 오버헤드가 제거되어 사용할 수 있는 리소스를 최대한 활용하여 쿼리를 실행한다.

<img src="assets/image 3-1.png" width="400"/>

Hadoop에서는 이와 같이 성질이 다른 쿼리 엔진을 목적에 따라 구분한다. 대량의 비구조화 데이터를 가공하는 무거운 배치 처리에 높은 처리량으로 리소스를 활용할 수 있는 Hive를 이용한다. 그렇게 해서 완성한 구조화 데이터를 대화식으로 집계하고자 할 때는 지연이 적은 Impala와 Presto등이 적합하다.

### Spark

Apache Spark 또한 MapReduce보다 더 효율적인 데이터 처리를 실현하는 프로젝트로 개발이 진행되고 있다. Hadoop의 연장선 상에 있는 Tez와 달리, Spark Hadoop과는 다른 독립된 프로젝트이다.

Spark는 Hadoop을 대체하는 것이 아니라 MapReduce를 대체하는 존재다. 예를 들어, 분산 파일 시스템인 HDFS나 리소스 관리자인 YARN등은 Spark에서도 그래도 사용할 수 있다. Hadoop을 이용하지 않는 구성도 가능하며, 분산 스토리지로 Amazon S3를 이용하거나 분산 데이터베이스인 카산드라에서 데이터를 읽어 들이는 것도 가능하다.

Spark의 실행은 자바 런타임이 필요하지만, Spark 상에서 실행되는 데이터 처리는 스크립트 언어를 사용할 수 있다. 표준으로 자바, 스칼라, 파이썬 그리고 R언어에 대응하고 있으며, 문서도 충실하기 때문에 도입하기 쉽다.

<img src="assets/image 4.png" width="400"/>

## 3-2 쿼리 엔진

### 데이터 마트 구축의 파이프라인

Hadoop에 의한 구조환 데이터의 작성과 이를 이용한 쿼리의 실행이 어떤 것인지를 알기 위해, 여기서는 실제로 ㅝ리 엔진을 사용하며 데이터 마트를 만들기까지의 흐름을 살퍼보자

<img src="assets/image 5.png" width="400"/>

분산 스토리지에 저장된 데이터를 구조화하고 열 지향 스토리지 형식으로 데이터를 저장한다. 이 작업은 여러개의 텍스트 파일을 읽어 들어 가공하는 부하가 큰 처리가 되기 때문에 Hive를 사용한다.

완성된 구조화 데이터를 결합, 집계하고 비정규화 테이블로 데이터 마트에 써서 내보낸다. 열 지향 스토리지를 이용한 쿼리의 실행에는 Presto를 사용함으로써 실행 시간을 단축할 수 있다.

### Hive로 비정규화 테이블을 작성하기

데이터의 구조화가 완료되면 다음은 데이터 마트의 구축이다. 테이블을 결합하여 비정규화 테이블을 만든다. 이때 Presto같은 대화형 쿼리 엔진을 사용할 것인지, Hive 같은 배치형 쿼리 엔진을 사용할 것인지에 따라 생각이 달라진다.

Hive와 Presto의 차이에 대해서는 나중에 언급하겠지만, 시간이 걸리는 배치 처리는 원칙적으로 Hive를 사용해야 한다. 예를 들어, 비정규화 테이블이 수억 레코드나 되면, 그것을 데이터 마트로 내보내는 것만으로도 상당한 시간이 소용된다. 따라서 쿼리 엔진의 성능은 최종 실행 시간에 그다지 많은 영향을 끼치지 않는다.

비정규화 테이블을 만드는 데 오랜 시간이 걸리는 것은 흔한 일이며, 그렇기에 가능한 효율적인 쿼리를 작성해야 한다.

### 서브 쿼리 안에서 레코드 수 줄이기

Hive 쿼리는 SQL과 매우 유사하지만, 그특성은 일반적인 RDB와는 전혜 다르다. Hive는 데이터베이스가 아닌 데이터 처리를 위한 배치 처리 구조이다. 읽을 데이터 양을 의식하면서 쿼리를 작성하지 않을 경우, 성능이 나오지 않을 수 있다.

```sql
SELECT ...
FROM access_log a
JOIN users b On b.id = a.user_d
WHERE b.created_at = '2017-01-01'

SELECT ...
FROM (
    SELECT * access_log
    WHERE time >= TIMESTAMP '2017-01-01 00:00:00'
) a
JOIN users b ON b.id = a.user_id
WHERE b.created-at = '2017-01-01'
```

전자와 달리 후자는 서브 쿼리 안에 팩트 테이블을 필터링하여 작게한다.

<img src="assets/image 6.png" width="400"/>

Hive가 쿼리를 최적화해주는 경우도 있으므로, 서브 쿼리화가 꼭 필요하지는 않지만 가능한 ‘초기에 팩트 테이블을 작게 하는 것’이 빅데이터의 집계에 중요하다.

예를 들어 최종적을 GROUP BY로 데이터를 집계하고 싶다면, 테이블을 결합하기 전에 서브 쿼리 안에서 집계할 수 있다. 데이터의 양을 감소시킨 후에 테이블을 결합하는 것이 쿼리 실행 시간을 단축할 수 있다.

### 데이터 편향 피하기

고속화를 방해하는 다른 문제는 ‘데이터의 편차’이다. 예를 들어 분산 시스템에서 SELECT를 실행하는 것은 다른 처리보다 시간이 오래 걸린다. 중복이 없는 깂을 세라면 데이터를 한곳에 모아야하기 때문이다.

---

# 4장 데이터의 축적

## 4-1 벌크 형과 스트리밍 형의 데이터 수집

### 객체 스토리지와 데이터 수집

빅데이터는 대부분의 경우 확장성이 높은 분산 스토리지에 저장한다. 분산 형의 데이터베이스가 이용되는 경우도 있지만, 우선 기본이 되는 것은 대량으로 파일을 저장하기 위한 객체 스토리지를 사용한다(HDFS, AWS S3)

<img src="assets/image -2.png" width="400"/>

객체 스토리지에서의 파일 읽고 끄기는 네트워크를 거쳐서 실행한다. 내부에서는 다수의 물리적인 서버와 하드디스크가 처리하고 있다. 데이터는 항상 여러 디스크 복사되기 때문에 일부 하드웨어가 고장 나더라도 데이터가 손실되지 않는다(Fault Tolerance)

객체 스토리지의 구조는 데이터양이 많을 때는 우수하지만, 소량의 데이터에 대해서는 비효율적이다. 통신 오버헤드가 크기 때문이다.

### 데이터 수집

빅데이터로 자주 다루는 것은 시계열 데이터, 즉 시간과 함께 생성되는 데이터인데, 그것을 수시로 객체 스토리지에 기록하면 대량의 작은 파일을 생성하여 시간이 지남에 따라 성능 저하의 요인이 된다.

반대로, 파일이 지나치게 커지는 것도 문제가 될 수 있다. 파일 크기가 증가하면 네트워크 전송에 시간이 걸려 예상하지 못한 오류 발생률도 높아진다. 1테라바이트의 파일을 100Mbp의 회선으로 전송하면 약 24시간이 소요된다. 거대한 데이터는 한번에 처리하는 것이 아니라 적당히 나눈ㅁ으로써 문제 발생을 줄일 수 있다.

빅데이터는 단지 수집만 해서는 안 되고 나중에 처리하기 쉽도록 준비해 둘 필요가 있다. 객체 스토리지에서 효율적으로 처리할 수 있는 파일 크기는 대략 1메가바이트에서 1기가바이트 사이의 범위다. 그것보다 작은 데이터는 모아서 하나로 만들고 큰 데이터는 복수로 나누는 것을 고려해본다.

수집한 데이터를 가공하여 집계 효율이 좋은 분산 스토리지를 만드는 일련의 프로세스를 데이터 수집이라고 한다. 여기에는 데이터 수집부터 구조화 데이터의 작성, 분산 스토리지에 대한 장기적인 저장등이 포함된다.

<img src="assets/image 1-2.png" width="400"/>

### 벌크 형의 데이터 전송

데이터 전송의 구조에는 벌크 형과 스트리밍 형 두가지 종류가 있다. 

전통적인 데이터 웨어하우스에서 사용된 것은 주로 벌크형 방식으로 데이터베이스나 파일 서버 또는 웹 서비스 등에서 각각의 방식으로 데이터를 추출한다. 빅데이터를 다루는 경우에도 과거에 축적된 대량의 데이터가 이미 있는 경우나 기존의 데이터베이스에서 데이터를 추출하고 싶은 경우에 벌크 형의 데이터 전송을 한다.

데이터가 처음부터 분산 스토리지에 저장되어 있지 않다면 ETL 서버를 구성한다. ETL 서버에는 구조화된 데이터 처리에 적합한 데이터 웨어하우스를 위한 ETL 도구와 오픈 소스의 벌크 전송 도구 또는 직접 작성한 스크립트 등을 이용하여 데이터를 전송한다.

<img src="assets/image 2-2.png" width="400"/>

### 파일 사이즈 적정화

벌크 형의 도구로 파일 사이즈를 적정화하는 것은 비교적으로 간단한다. ETL 프로세스는 스케줄링 태스크를 실행하므로 이때 축적된 데이터를 하나로 모은다

전송 방법을 검토하는 것도 좋은 방법이다. 100개의 파일을 전송하는데 100번 전송을 반복하고 있다면 모아서 전송하는 것이 좋다.

데이터양이 많을 때는 한 달씩이나 하루 단위로 전송하도록 작은 태스크로 분해하여 한번의 태스크 실행이 커지지 않도록 조정한다. 워크플로 관리 도구를 사용하면 이러한 태스크 실행을 쉽게 관리할 수 있다. 태스크를 비교적 작게 함으로써 디스크가 넘쳐나는 잠재적인 문제를 해결할 수 있음은 물론이고 만약 도중에 문제가 발생해도 재시도하기가 쉬워진다.

### 데이터 전송의 워크플로

데이터 전송이 신뢰성이 중요한 경우에는 가능한 한 벌크 형 도구를 사용해야 한다. 스트리밍 형의 데이터 전송은 나중에 재실행하기가 쉽지 않다. 문제가 발생했을 때 여러 번 데이터 전송을 재실행할 수 있다는 점이 벌크 형의 장점이다.

벌크 형의 데이터 전송은 워크플로 관리 도구와의 궁합이 뛰어나다. 스트리밍 형 전송은 그 특성상 실시간으로 계속 동작하는 것을 전제로 하므로 워크플로의 일부로 실행하지 않는다. 과저의 데이터를 빠짐없이 가져오거나 실패한 작업을 재실행할 것을 고려한다면, 벌크 형 전송을 해야 한다.

위와 같은 특성으로 벌크 형 데이터 전송은 워크플로 관리 도구와 함께 도입한다. 정기적인 스케줄 실행 및 오류 통지 등은 워크플로 관리 도구에 위임한다.

### 스트리밍 형의 데이터 전송

방금 생성되어 아직 저장되지 않은 데이터는 바로 전송해야한다. 대부분의 데이터는 통신 장비 및 소프트웨어에 의해 생성되고, 네트워크를 통해 전송된다. 이러한 데이터를 벌크 형 도구로 모으는 것은 불가능하므로, 스트리밍 형의 데이터 전송이 필요하다. 

<img src="assets/image 3-2.png" width="400"/>

이러한 데이터 전송의 공통점은 다수의 클라이언트에서 계속해서 작은 데이터가 전송되는 것이고, 이와 같은 전송 방식은 메세지 배송이라고 한다. 메세지 배송 시스템은 전송되는 데이터양에 비해 통신을 위한 오버헤드가 커지기 때문에 이를 처리하는 서버는 높은 성능을 요구한다.

받은 메세지를 저장하는 데에는 몇가지 방법이 있다. 그중 하나는 작은 데이터를 쓰기에 적합한 NoSQL 데이터베이스를 이용하는 것이다. 이 경우 Hive와 같은 쿼리 엔진으로 NoSQL 데이터베이스에 연결해 데이터를 읽을 수 있다.

또는 분산 스토리지에 직접 쓰는 것이 아니라 메세지큐와 메세지 브로커와 같은 중계 시스템에 전송할 수 있다. 이 경우 받은 데이터를 일정한 간격의 꺼내어 모아 함께 분산 스토리지에 저장한다.

### 웹 브라우저에서의 메세지 배송

자체 개발한 웹 애플리케이션에서는 웹 서버 안에서 메세지를 만들어 배송한다. 전송 효율을 높이기 위해 서버상에서 일단 데이터를 축적해 높고 나중에 모아서 보내는 경우가 많다. 이때는 Fluentd, Logstash와 같은 서버 상주형 로그 수짖ㅂ 스프트웨어가 자주 사용된다.

다른 방법으로는 자바 스크립트를 사용하여 웹 브라우저에서 직접 메세지를 보내는 경우도 있다. 이것은 웹 이벤트 추적이라고 한다.

<img src="assets/image 4-2.png" width="400"/>

## 4-2 [성능X신뢰성] 메서지 배송의 트레이드 오프

클라이언트의 수가 많아지면 스트리밍 형의 메세지 배송의 성능과 신뢰성을 둘다 만족하기는 어렵게 된다. 메세지 브로커 중심으로 하는 메세지 배송의 구조와 한계에 관해 살펴본다.

### 메세지 브로커

메세지 배송에 의해 보내진 데이터를 분산 스토리지에 저장할 때는 주의가 필요하다. 데이터 양이 적을 때에는 문제가 되지 않지만, 쓰기의 빈도가 증가함에 따라 디스크 성능의 한계에 도달해 더 쓸 수 없게 될 우려가 있기 때문이다. 특히, 외부에서 보내오는 메세지의 양을 제어할 수 없기 때문에 급격한 데이터양 증가에 대응하는 것은 쉬운 일이 아니다.

만약 쓰기 성능의 한계에 의해 오류가 발생하면, 대부분의 경우 클라이언트는 메세지를 재전송하려고 한다. 하지만 성능적인 한계에 도달한 상황에서는 아무리 재전송해도 부하가 높아지지만 할 뿐 아무것도 해결되지 않는다. 결국 쓰기 성능을 높이거나 클라이언트가 부하가 떨어질 때가지 기다리는 수밖에 없다

대량의 메세지를 안정적으로 받기 위해서는 빈번한 쓰기에도 견딜 수 있도록 성능이 매우 높고 필요에 따라 성능을 얼마든지 올릴 수 있는 스토리지가 필요하다. 분산 스토리지가 반드시 그런 성격을 가지고 있다고는 할 수 없기 때문에 빅데이터의 메세제 배송 시스템에서는 종종 데이터를 일시적으로 축적하는 중산층이 설치된다. 이것을 메세지 브로커라고 한다.

<img src="assets/image 5-2.png" width="400"/>

빅데이터를 위한 분산형 메세제 브로커는 오픈 소스의 경우 Apache Kafka가 유명하고, 클라우드 서비스라면 Amazon Kinesis가 유명하다.

### 푸쉬 형과 풀형

송신 촉의 제어로 데이터를 보내는 방식이 푸시형이라고 하고, 수신 측의 주도로 데이터를 가져오는 것을 풀형이라고 한다. 메세지 브로커는 데이터의 쓰기 속도를 조정하기 위한 완충 부분이며, 푸쉬 형에서 풀 형으로 메세지 배송의 타이밍을 변환한다.

메세지 브로커에 데이터를 넣는 것을 생산자, 꺼내오는 것을 소비자라고 한다. 메세지 브로커는 높은 빈도로 데이터를 쓴느 것에 최적화되어 있으며, 여러 대의 노드에 부하 분산함으로써 성능을 끌어 올릴 수 있는 뛰어난 확장성을 구현하고 있다. 따라서 푸쉬 형의 메세지 배송은 모두 메세지 브로커에 집중시키고 거기에서 일정한 빈도로 꺼낸 데이터를 분산 스토리지에 기록하여 성능 문제를 파악 할 수 있다.

풀 형의 메세지 배송은 파일 사이즈 적정화에도 도움이 된다. 프론트엔드는 대량의 메세지를 받기 때문에 그대로 저장하면 매우 많은 작은 파일이 생성된다. 소비자는 메세지 브로거로부터 일정한 간격으로 데이터를 취함으로써 적당히 모아진 데이터를 분산 스토리지에 기록한다.

### 메세지 라우팅

구체적인 숫자로 생각해보자. 어떤 시스템이 100만 대의 디바이스에서 1분마다 100바이트의 메세지를 수신한다고 하자. 시스템 전체가 받는 메세지 초당 1.7만 메세지, 데이터양으로 치면 1.66MB이다. 데이터양만 보면 그리 많지는 않지만 초당 1.7만 번의 쓰기에 견뎌야 하는 데이터 베이스를 준비하기는 쉽지 않다.

<img src="assets/image 6-2.png" width="400"/>

따라서 프론트 엔드에서는 메세지 브로커에 데이터를 푸쉬하도록 하고 그것을 소비자에서 모아서 가져온다. 가령 1초마다 가져오면 한 번에 가져오는 데이터의 양은 1.66MB이므로 이를 실시간으로 처리하는 것은 그다지 어렵지 않다. 이렇게 짧은 간격으로 차례대로 데이터를 꺼내서 처리하는 것 스트림 처리다

메세지 브로커에 써넣은 데이터는 복수의 다른 소비자에서 읽어 들일 수 있다. 이를 통해 메세지가 복사되어 데이터를 여러 경로로 분기시킬 수 있다. 이것을 메세지 라우팅이라고 한다. 예를 들어, 메세지 일부를 실시간 장애 감지를 사용하면서 같은 메세지를 장기적인 데이터 분석을 위한 분산 스토리지에 저장하는 것도 가능하다.

### 메세지 배송을 확실하게 실시하는 것은 어렵다.

성능 문제 외에도 피할 수 없는 것이 신뢰성의 문제다. 특히 모바일 회선과 같은 신뢰성이 낮은 네트워크에서는 반드시 메세지의 중복이나 누락이 발생한다. 장애를 어떻게 처리할지는 시스템에 따라 다르다. 대부분의 경우는 다음 중 하나를 보장하도록 설계된다.

- at most once: 메세지는 한번만 전송된다. 그러나 도중에 전송에 실패해서 사라질 가능성이 있다.
- exactly once: 메세지는 손실되거나 중복 없이 한 번만 전달된다
- at least once: 메세지는 확실히 전달된다. 단 같은 것이 여러 번 전달된 가능성이 있다.

| 소프트웨어 | 신뢰성 |
| --- | --- |
| Apache Flume | at least one |
| Apache Kafka | at least once |
| Logstash | at least once |
| Fluentd | option at least once |

### 중복 제거는 높은 비용의 오퍼레이션

메세지의 중복을 제거하려면 같은 메세지를 과거에 받은 것인지에 대한 여부를 판정해야 한다. TCP에서는 메세지에 시퀀스 번호를 붙이고 있지만, 분산 시스템에서는 시퀀스 번호는 사용하지 않는다.

### 오프셋을 이용한 중복 제거

파일 전송의 방식과 유사한 방법으로 전송해야할 데이터에 파일명 들의 이름을 부여해 그것을 작은 메세지에 실어서 배송한다. 각 메세지에는 파일 안의 시작 오프셋을 추가한다. 만일 메세지가 중복되어도 같은 파일의 같은 오프셋을 덮어쓰기 때문에 무방하다. 이 방법은 벌크 형의 데이터 전송과 같이 데이터 양이 고정된 경우에는 잘 작동한다. 한편 스트리밍 형식의 메세지 배송에는 이 방식을 채용하고 있는 경우는 거이 없다.

### 고유 ID에 의한 중복 제거

스트리밍 형의 메세지 배송에서 자주 사용되는 것은 모든 메세지에 UUID와 같은 고유 ID를 지정하는 방법이다. 이 경우 메세지가 늘어남에 따라 ID가 폭발적으로 증가하므로 그것을 어떻게 관리하느냐가 문제다. 과거에 전송된 모든 ID를 기억하는 것은 비현실적이고, 그렇다고 ID를 파기하면 늦게 도착한 메세지가 중복된다.

현실적으로는 최근에 받은 ID만을 기억해두고 그보다 늦게 온 메세지의 중복은 허용한다.

### 종단간(End to End)의 신뢰성

중복도 결손도 없는 신뢰성이 높은 메세지 배송을 실현하는 것은 쉬운 일이 아니므로 모든 소프트웨어에서 충분한 신뢰성이 실현되어 있다고는 기대할 수 없다. 메세지 배송에 있어서 성능과 신뢰성은 트레이드 오프의 관계에 있으므로 한쪽을 우선하면 다른 한쪽이 희생된다.

빅데이터 메세지 배송에서는 종종 신뢰성보다 효율쪽이 중시된다. 따라서, 중간 경로에 at least once를 보장하는 반면, 중복 제거는 하지 않는 것이 포준적인 구현이다. 원래 중복 제거란 종단 간에 실행하지 않으면 의미가 없다. 즉, 클라이언트가 생성한 메세지를 최종 도달 지점인 분산 스토리지에 기록하는 단계에서 중복 없는 상태로 해야한다.

지금까지 봐 온 것처럼 스트리밍 형의 메세지 배송은 클라이언트에서 프론트엔트, 그리고 메세지 브로커와 소비자를 포함한 다수의 요소로 구성된다. 그중 일부분에서 중복 제거가 실현되더라도 다른 곳에서 중복이 발생할 수 있다.

메세지 배송의 최종적인 신뢰성은 중간 경로의 신뢰성 조합으로 결정된다. 중간에 한부분이라도 at most once가 있으면 메세지를 빠뜨릴 가능성이 있고, at least once가 있으면 중복될 가능성이 있다. 신뢰성이 높은 메세지 배송을 실형하려면 중간 경로를 모두 at least once로 통일한 후 클라이언트 상에서 모든 메세지에 고유 ID를 포함하도록 하고 경로의 말단에서 중복 제거를 실행해야한다.

### 데이터 수집의 파이프라인

이처럼 일련의 프로세스를 거친 다음, 마지막으로 데이터를 구조화해서 열 지향 스토리지로 변환함으로써 마침내 장기간의 데이터 분석에 적합한 스토리지가 완성되고 “데이터 수집의 파이프라인”

<img src="assets/image 7-2.png" width="400"/>

실제로 어떤 파이프라인을 만들지는 요구 사항에 달려 있으므로 필요에 따라 시스템을 조합한다. 쓰기 성능에 불안감이 없다면 메세지 브로커는 불필요하므로 클라이언트나 프론트엔드에서 NoSQL 데이터베이스에 직접 데이터를 쓰는 것도 좋다. 다소 중복이 허용된다면 중복 제거도 생략할 수 있다.

데이터 집계에 쿼리 엔진을 사용하는 경우에는 구조화된 데이터를 열 지향 스토리지형식으로 객체 스토리지에 저장한다. MPP 데이터베이스를 사용하고 있다면 정기적을 데이터를 로드하면 완성이다.

### 중복을 고려한 시스템 설계

일반적을ㅗ스트림 형의 메세지 배송에서는 중간에 명시적으로 중복 제거 방식을 도입하지 않는 한 항상 중복의 가능성이 있다고 생각하는 것이 좋다. 빅데이터를 다루는 시스템은 매우 높은 성능을 요구하기 때문에 아주 작은 중복은 무시하는 경향이 있다.

신뢰성이 중시되는 경우에는 스트리밍 형의 메세지 배송을 피하는 것이 가장 좋다. 예를 들어 과금 데이터처럼 오차가 허용되지 않는 것은 트랜잭션 처리를 지원하는 데이터베이스에 애플리케이션이 직접 기록해야 한다. 그런 후에 벌크 형의 데이터 전송을 함으로써 중복도 결손도 확실하게 피할 수 있다.

## 4-3 시계열 데이터의 최적화

스트리명 형의 메세제 배송에서는 메세지가 도착할 때까지의 시간 지연이 문제이다. 이번에는 늦게 도달하는 데이터가 집계 속도에 미치는 영향에 관해 살펴보자

### 프로세스 시간과 이벤트 시간

스마트 폰에서 데이터를 수집하면 메세지가 며칠 늦게 도착하는 일은 드물지 않다. 사용자가 전파가 닿지 않는 곳을 외출하거나 배터리가 완전히 방전될 수도 있기 때문이다. 모바일 앱에 따라서는 화면이 전환되면 다음에 애플리케이션이 시작될 때까지 메세지가 전송되지 않는 경우도 있다.

클라이언트 상에서 메세지가 생성된 시간을 이벤트 시간, 서버가 처리하는 시간을 프로세스 시간이라고 한다. 종종 데이터 분석의 대상이 되는 것은 이벤트 시간이기 때문에 이 시간의 차이가 문제를 일으킨다.

### 프로세스 시간에 의한 분할과 문제점

예를 들어, 모바일 앱의 활동 사용자 수를 집계하는 것을 생각해보자. 늦게 도달하는 데이터가 있다는 것은 과거의 집계 결과가 매일 조금씩 바뀐다는 것을 의미한다. 보다 실태에 가까운 집계 결과를 얻기 위해서는 이벤트 시간보다 며칠 정도 지난 지점에서 집계해야한다.

<img src="assets/image 8-2.png" width="400"/>

한편 분산 스토리지에 데이터를 넣는 단계에서는 이벤트 시간이 아니라 프로세스 시간을 사용하는 것이 보통이다. 예를 들어 2017년 1월 1일에 도착한 데이터는 20170101와 같은 이름을 지정한다. 파일에는 이벤트 시간을 ㅗ보면 다수의 과거 데이터가 포함된 상태이다

<img src="assets/image 9-2.png" width="400"/>

이 상태에서 과거 특정 일이 발생한 이벤트를 집계하고 싶다고 하자. 예를 들어 1월 1일에 발생한 이벤트라면 그 이후에 만들이진 모든 파일에 포함되어 있을 수 있다. 1개월 후인 2월 1일에 지금까지 만들어진 모든 파일을 열고 기기에서 1월1일의 데이터만 뽑아내면 비교적 정확한 결과를 얻을 수 있다.

그러나 한 달 사이에 만들어지는 수십만 파일 중에서 특정 일의 데이터만을 찾는다는 것은 매우 시간과 자원을 낭비하는 처리다. 이런 일이 일어나는 원인은 데이터가 이벤트 시간으로 정렬되어 있지 않아 모든 데이터를 로드하고 검색하는 풀 스캔을 필요로하기 때문이다.

이러한 방법을 해결하기 위해서는 RDB의 경우 시계열 인덱스를 사용하고, 열지향 스토리지는 조건절 푸시 다운을 사용할 수 있다.