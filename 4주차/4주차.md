### 워크플로우 도구가 필요한 이유

데이터 파이프라인이 복잡해지거나, 실행되는 개별 처리인 ‘태스크’ 수가 증가하면, 태스크를 다시 실행하는 일이 어려워진다.

이때 필요한 것이 워크플로우 관리 도구고 다음과 같은 기능을 제공한다.

1. 태스크를 정기적인 스케줄로 실행하고, 결과 통지하기
2. 태스크간 의존 관계를 정하고 정해진 순서대로 실행하기
3. 태스크간 실행 결과를 보관하고 오류 발생시 재실행을 도와주기

워크플로 관리 도구는 선언형과 스크립트형으로 나뉜다.

선언형 도구보다 스크립트형 도구는 더 유연하고, 태스크 정의를 프로그래밍할 수 있다. 

빅데이터를 취급하면 네트워크의 일시적 장애, 하드웨어 장애, 스토리지 용량 부족, 쿼리 증가에 따른 성능 부족 등 다양한 오류가 발생한다. 미리 예기치 못한 오류가 발생할 가능성을 고려하여 대처 방법을 결정해두는 것이 중요하다.

워크프로 관리에선, 태스크 실행 순서와 장애시 어떻게 회복할지에 대한 계획을 정한다.

### 원자성

태스크 도중까지 실행하다 실패시 경과가 사라지지 않고 남는다면, 재실행에 의한 데이터가 혼재하는 문제가 발생한다. 따라서 마지막까지 성공하거나, 실패시 아무것도 남지 않는 두가지 상황만 허가한다.

### 트랜잭션 처리와 원자성 조작

1. 트랜잭션 처리에 대응한 데이터베이스라면, 여러번의 쓰기를 한번의 트랜잭션으로 실행할 수 있다
2. 그렇지 않다면, 쓰기가 필요한 수만큼 태스크를 나눠 ( 원자성 조작 ) 재시도의 안정성을 높일 수 있다.

( 하지만, 태스크 수행 후 통신이 끊겨 오류가 발생한 상황이 생긴다면 워크플로 입장에선 장애, 따라서 원자성 조작에 의존한 플로우를 만들어선 안된다 )

### 데이터 중복 문제

문제 : 태스크 중간에 실패 후 재실행되면, **중복된 데이터가 삽입**될 수 있다.

**해결책 1: 원자성 조작 →** 하나의 태스크는 **전체가 실행되거나, 전혀 실행되지 않도록** 한다.

하지만, 태스크 수행 후 통신이 끊겨 오류가 발생한 상황이 생긴다면 워크플로 입장에선 장애, 따라서 원자성 조작에 의존한 플로우를 만들어선 안된다

**해결책 2: 멱등한 조작 →** 같은 태스크를 **여러 번 실행해도 동일한 결과**가 되도록 만든다.

- **추가 (Append)**: 매번 다른 데이터 덧붙이기 → 중복 위험 존재.
- **치환 (Replace)**: 기존 데이터 삭제 후 새로 추가 → 멱등 보장됨.

**문제**: 현실적으로 항상 테이블 전체를 치환할 수는 없다.

`INSERT`로 그날 데이터만 추가해야 할 때 → 원자성은 보장되지만 멱등성은 깨짐.

**해결책1**

- 데이터를 **시간 단위(예: 일별, 시간별)로 파티셔닝 →** 각 파티션 단위로 **치환 (TRUNCATE + INSERT)** 진행

해결책 2

- **중간 테이블**에 매번 덮어쓰기 (DROP + CREATE → 멱등함)
- 시간 단위로 테이블을 파티셔닝 후 치환(TRUNCATE + INSERT)

### 병렬 실행을 위한 태스크 큐

- **너무 작으면**: 오버헤드 증가, 성능↓, 오류↑
- **너무 크면**: 병렬 실행 어려움, 실패시 리스크↑

### 배치형 데이터 플로우 - mapReduce 보단 DAG

Map과 Reduce는 한 번에 하나씩 순차 실행됨 → 병렬성 낮고 대기 시간 발생

대안으로 DAG이 있다.

- Lazy Evaluation (지연 평가) : DAG는 미리 만들되, **실제로 실행은 결과를 요청할 때 시작**
- 의존 관계만 명확히 하면, **병렬로 실행할 수 있는 태스크를 한 번에 실행 가능**

### 데이터 플로우와 워크플로우의 통합

- DAG 기반 데이터 플로우만으로는 **에러 복구, 재시도** 설계가 어렵다.
- 따라서 실무에서는 반드시 **워크플로우 관리 도구(Airflow, Prefect 등)** 함께 사용

### 읽기 데이터 플로우

**데이터 소스를 직접 읽지 말고**, 먼저 **분산 스토리지로 복사**해서 사용한다.

<img width="687" height="230" alt="스크린샷 2025-08-06 오후 6 36 01" src="https://github.com/user-attachments/assets/d1e420cb-c16f-418a-889b-795832fa2d42" />


### 쓰기 데이터 플로우

**외부 시스템에 쓰는 작업은 반드시 **마지막 단계에서 CSV 등으로 내보내기**

<img width="677" height="239" alt="스크린샷 2025-08-06 오후 6 36 23" src="https://github.com/user-attachments/assets/ca91bec4-34ac-4cdc-990d-6ca3a3312504" />

### 스트리밍형 데이터 플로우

받은 데이터를 분산 스토리지에 보관하는 부분부터 시작하는 것이 배치처리 방식이라면, 분산 스토리지를 거치지 않고 계속하는 것이 스트림 처리 방식이다.

과거의 데이터 집계하고 싶은지, 실시간 데이터에 관심 있는지에 따라 적절한 방식 선택하면 된다.

<img width="673" height="249" alt="스크린샷 2025-08-06 오후 6 36 49" src="https://github.com/user-attachments/assets/4b09c650-fbd5-49d5-b6dc-d3a8880f55bb" />

### 스트림 처리의 두가지 문제에 대한 대처

1. 틀린 결과를 어떻게 수정할까?
2. 늦게 전송된 데이터를 어떻게 취급할까?

### 해결책

1. 람다 아키텍쳐
2. 카파 아키텍쳐


### 데이터 웨어하우스의 병목 현상을 해결해준 구글 BigQuery ( ㅁㄹㅋㄹ 사례 )

- 기존 파이프라인 ( 로그기반 cdc를 데이터 웨어하우스에 적재 )

소스 데이터 베이스가 여러개, 데이터베이스에서 발생한 변경 사항을 감지하는 방법 중 로그 **기반 CDC를 데이터 분석가들이 보는 데이터 웨어하우스에 적재하는 파이프라인**

데이터 → amazon s3 → airflow → data warehouse

<img width="698" height="292" alt="스크린샷 2025-08-06 오후 6 37 14" src="https://github.com/user-attachments/assets/90421a84-ed6c-4666-841d-690aed8469b6" />

< 문제점 >

1. Amazon S3에서 파일을 읽어와 CDC 데이터를 DW 비교 후 적재 →  **I/O 발생**
2. **Airflow 기반 적재 작업과 분석 쿼리**가 **동시에 실행되며 동일한 자원**을 사용해 경쟁이 발생한다.
3. 스크립트를 통하여 **Data Warehouse 테이블**과 **AWS S3**에 있는 CDC 로그 데이터를 비교하여 **UPSERT(Update + Insert) 방식**으로 데이터를 **적재하였다. 
→** 에어플로우(Airflow)에는 명시적인 `merge` 연산은 없다고 한다
→ **UPSERT는 Delete 작업**이 반영되지 않아 원본 테이블과 **정합성이 맞지 않는 경우가 생긴다.**


- BigQuery 도입 후 파이프라인
<img width="701" height="377" alt="스크린샷 2025-08-06 오후 6 37 47" src="https://github.com/user-attachments/assets/7847da39-e7b4-44c4-b4c5-cf3064f6f39b" />

BigQuerystreaming API를 사용하여 별도의 파일 시스템도, 객체 스토리지인 amazon s3에 접근하여 데이터를 읽는 I/O도 없도록 하여 더 빠른 데이터 저장이 가능해졌다. 

BigQuery는 **Merge 문**을 사용하여 Insert, Update, Delete를 스크립트를 사용할 필요 없이 빠른 속도로 데이터를 적재했으며, **데이터 연동 지연 시간이 대폭 감소**했습니다.

### 정리 - airflow 역할 변화, bigQuery 기반 구조 변화

기존엔 airflow DAG 는

1. S3에 쌓인 CDC 로그 읽음
2. 기존 DW 테이블과 비교하며 변경사항 감지
3. UPSERT 연산

BigQuery 도입 후 airflow는

1.  CDC Log Table과 Final Table 간의 merge쿼리를 트리거하는 역할
