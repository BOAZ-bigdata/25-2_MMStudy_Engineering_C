# 6-1. 스키마리스 데이터의 애드 혹 분석

## 6-1-1. 스키마리스 데이터 수집하기

- 데이터 소스: Twitter 스트리밍 API
    
    ![image.png](./assert/6장/image%201.png)
    
- 분산 스토리지: MongoDB

### 테스트 환경 구축

1. 자바 버전 확인 후 MongoDB 설치 및 시작
    
    ![image.png](./assert/6장/image%202.png)
    
2. Twitter 스트리밍 API를 호출해 MongoDB에 적재하는 스크립트
    
    ![image.png](./assert/6장/image%203.png)
    

## 6-1-2. 대화식 실행 환경의 준비

- Jupyter Notebook으로 수집한 데이터를 대화식으로 보는 환경 준비
- MongoDB의 클라이언트 라이브러리로 원시 데이터를 참조한 뒤, pandas를 활용해 DataFrame 형식으로 변환

## 6-1-3. Spark에 의한 분산 환경

- 데이터양이 늘어도 대응 가능하도록 spark를 사용해 분산 처리를 진행

### MongoDB의 애드 혹 집계

- MongoDB로부터 데이터를 읽어들이기 위해 Spark 세션을 통해 데이터 프레임을 작성한다
    - Spark에서도 데이터 프레임은 표 형식의 데이터를 추상화한 객체
    - (앞서 데이터 플로우에서 dag 정의 후 조립만 진행하던 것처럼) 작성 후 무언가 처리를 요구하기 전까지는 아무런 처리도 하지 않는다
- SparkSQL 사용 시, 데이터 프레임을 SQL로 집계 가능하다
    
    ![image.png](./assert/6장/image%204.png)
    
- MongoDB의 경우 칼럼 단위의 읽기에 최적화되어 있지 않아 고속 집계에 적합하지 않기 때문에 최적화를 위해서는 한 차례의 데이터 추출이 필요하다

### 텍스트 데이터의 가공

- Spark는 스크립트 언어에 의해 프로그래밍이 가능하다는 것이 최대 장점으로 꼽힌다
- SQL로 필요한 데이터를 추출하고, 파이썬을 활용한 간편한 전처리가 가능하다

### Spark 프로그램에 있어서의 DAG 실행

- Spark에서는 RDD라고 불리는 로우 레벨의 데이터 구조를 사용하면 MapReduce와 마찬가지로 임의의 함수를 Map과 Reduce로 적용 가능하다
    - RDD(Resilient Distributed Dataset): 로우 레벨의 데이터 구조
- flatMap()을 호출해 텍스트를 단어로 분해해 데이터 프레임으로 만들고(Map), SQL로 카운트(Reduce)할 수 있다

![image.png](./assert/6장/image%205.png)

![image.png](./assert/6장/image%206.png)

## 6-1-3. 데이터를 집계해서 데이터 마트 구축하기

- 데이터 마트를 만들기 위한 선택지
    1. Spark에 ODBC/JDBC로 접속하기
    2. MPP 데이터베이스에 비정규화 테이블 만들기
    3. 데이터를 작게 집약하여 CSV 파일에 출력하기
- word 열의 값을 카운트하여 등장 횟수가 1,000 이하인 단어를 카테고리화하는 방식으로 테이블의 레코드 수를 줄임으로써 카디널리티를 줄일 수 있다
- Spark에서 CSV에 결과를 출력하는 방법
    
    → 표준 spark-csv 라이브러리 사용
    
    - Spark 클러스터 상에서 실행되어 소량의 데이터를 출력하기에는 과하기 때문에 pandas를 사용하는 것이 간단하다

## 6-1-4. BI 도구로 데이터 시각화하기

- 노트북에서 데이터를 원하는 대로 만들고, BI 도구에 시작화를 진행하는 것이 간편하다
- 일회성이 아니라 계속해서 모니터링을 해야한다면 이러한 절차를 워크플로화하는 것을 검토해야 한다

# 6-2. Hadoop에 의한 데이터 파이프라인

## 6-2-1. 일일 배치 처리를 태스크화하기

- 데이터 소스: MongoDB
- 장기적인 데이터 분석을 위해 Hive로 열 지향 스토리지를 만들고, Presto로 집계
- 1일 단위로 파티셔닝된 시계열 테이블 생성

![image.png](./assert/6장/image%207.png)

## 6-2-2. [태스크 1] Embulk에 의한 데이터 추출

- Embulk: 오픈 소스 벌크 전송 도구
- Embulk를 사용해 MongoDB로부터 데이터를 추출한다 (JSON으로)
- 태스크의 파라미터로 시간의 범위를 1일씩으로 설정하여 정기적으로 데이터 전송이 실행되도록 한다

## 6-2-3. [태스크 2] Hive에 의한 데이터 구조화

- 앞서 작성한 JSON 파일을 Hive로 시계열 테이블로 만든다
- 파티셔닝이 가능하도록 하며, 매일 하루 단위로 새로운 파티션이 만들어지도록 한다

## 6-2-4. [태스크 3] Presto에 의한 데이터 집계

- Presto로 Hive 메타스토어를 참고해 데이터를 가져와 쿼리를 실행할 수 있다
- 데이터를 집계하여 CSV 파일(데이터 마트)로 추출한다

# 6-3. 워크플로 관리 도구에 의한 자동화

## 6-3-1. Airflow

- Apache Airflow는 파이썬으로 워크플로를 기술할 수 있는 스크립트 형 도구이자 오픈 소스 워크플로 관리 도구이다
- 워크플로 관리 콘솔은 웹으로 실행 가능하다

### 워크플로의 정의

- Airflow에 의한 워크플로는 여러 태스크로 이루어진 DAG의 형태로 정의한다
- 의존 관계가 없는 태스크는 병렬로 동시에 실행된다
- 각 DAG에는 어떤 스케줄로 각각 실행할지 반드시 지정한다

### 태스크의 정의

- Operator: 태스크를 만들기 위한 클래스
- Airflow에는 많은 플러그인이 제공되어 다른 서비스에 접근할 수 있는 다양한 Operator를 활용 가능하다
    - Hive, MySQL, AWS S3 등등…
    - python 스크립트 → PythonOperator
    - 셸 스크립트 → BashOperator

## 6-3-2. 워크플로를 터미널로 실행하기

- `airflow test` : 개별 태스크를 테스트 → 실행 결과 기록 X
- `airflow backfill` : DAG에 포함되는 모든 태스크 실행 → 실행 결과 기록 O

## 6-3-3. 스케줄러를 기동하여 DAG를 정기 실행하기

- `airflow scheduler` : 태스크의 스케줄 실행을 시작 ⇒ 스케줄러를 가동
    - 스케줄러는 DB 상태를 정기적으로 점검하고, 실행 가능한 태스크를 찾아 워크 프로세스에 전달하여 실행
- 오류가 난 경우 웹 콘솔을 통해 태스크의 상태와 함께 확인할 수 있다
- 모든 DAG는 실행 간격(schedule_interval)과 시작 시간(start_date)을 지정해야 한다
    - 주의) 일별로 진행되는 스케줄인 경우 1월 1일에 대한 태스크는 1월 2일이 되는 순간 실행된다

## 6-3-4. 태스크가 소비하는 자원 제어하기

- 설정 파일을 통해 스케줄러가 동시 실행하는 태스크 수의 상한을 조절할 수 있다
- CeleryExecutor, MesosExcutor 등의 구조를 통해 Airflow에서도 분산 처리가 가능하다

# 6-4. 클라우드 서비스에 의한 데이터 파이프라인

## 6-4-1. 데이터 분석과 클라우드 서비스의 관계

- 빅데이터를 다루는데 필요한 시스템 자원은 매일 변동하며, 실시적으로 평소의 몇 배나 되는 처리 능력이 요구되는 경우가 꽤 존재한다. 이때 높은 확장성을 제공하는 클라우드 서비스가 필수적이다
- 풀 매니지드 형 서비스: 어느정도 장애 대응과 확장성의 확보까지 서비스 일부로 제공되는 경우

## 6-4-2. 아마존 웹 서비스(AWS)

- 주요 서비스
    - Amazon Redshift: 데이터 웨어하우스를 위한 MPP 데이터베이스
    - Amazon S3: 장기적인 데이터 보존에 적합한 분산 스토리지 (객체 스토리지)
    - S3 → Redshift
        - 기본적으로 COPY 명령으로 로드
        - Redshift Spectrum을 이용해 S3 상의 파일을 Redshift에서 직접 편집 가능
- 특징:
    - 기능마다 세세히 나눠진 서비스로 조합의 자유도가 높다 (전체를 제대로 연결하기에는 어느정도 능력치가 요구됨)

## 6-4-3. 구글 클라우드 플랫폼(GCP)

- 구글의 소프트웨어와 기본 시설을 활용하여 대규모 데이터 처리를 실행 가능하다는 강점
- BigQuery: 풀 매니지드 형의 뎅데이터 웨어하우스 서비스
    - 분산 스토리지와 쿼리 엔진이 분리된 아키텍처
- 특징적인 서비스
    - Google Cloud Dataflow: 데이터 플로우의 방식을 서비스화
    - Google Cloud Datalab: 노트북을 서비스화
    
    → 모두 같은 데이터 센터에서 실행함으로써 고속 네트워크로 연결된 데이터 분석 환경 구축 가능
    
- 애드 혹 데이터 분석과 처리 속도에 대한 요구가 큰 경우에게 좋은 선택지

## 6-4-4. 트레주어 데이터

- 데이터 처리 플랫폼
- AWS, GCP와 다르게 처음부터 모든 서비스가 포함된 상태로 제공
- 특징:
    - 다수의 외부 시스템과의 연계가 통합되어 있음
        
        → 외부에서 데이터를 가져오고, 결과를 외부에 출력함으로써 더 큰 데이터 파이프라인 조립 가능
        
- Digdag
    - 오픈 소스 선언형 워크플로 관리 도구
    - yaml 표기법으로 태스크 정의
        - 수행하고 싶은 태스크의 내용은 별도의 스크립트나 SQL 파일로 준비
    - 정의한 워크플로는 서비스에 업로드해 클라우드 상에서 실행되며, 직접 서버를 유지 관리할 필요 없이 데이터 파이프라인 전체를 클라우드화 가능