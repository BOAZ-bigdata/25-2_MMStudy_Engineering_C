# 스키마리스 데이터의 애드 혹 분석
## 스키마리스 데이터 수집하기
- 스트리밍 APIs를 이용
## Spark에 의한 분산 환경
- spark 세션을 통해 데이터 프레임을 작성
- spark는 스크립트 언어에 의한 프로그래밍이 가능
- spark 임의의 함수를 MapReduce처럼 Map과 Reduce로 적용할 수 있다
## 데이터를 집계해서 데이터 마트 구축하기
- 카디널리티의 삭감 => 등장횟수가 일정 이하인 단어를 새로운 카테고리로 분류
- CSV 파일 작성 => 표준 라이브러리 사용 또는 pandas 데이터 프레임

# Hadoop에 의한 데이터 파이프라인
## 일일 배치 처리를 태스크화하기
- 벌크형 데이터 전송 > 분산 시스템 > 데이터 구조화 > 쿼리 엔진
## Embulk에 의한 데이터 추출
- 지정된 기간의 데이터를 추출(파라미터 부여)함으로써 멱등하다
## Hive에 의한 데이터 구조화
- 데이터 구조화 시에는 INSERT OVERWRITE TABLE을 이용해 파티션을 덮어쓰므로 태스크 멱등
## Presto에 의한 데이터 집계
- 워크플로 내부에서 호출해도 문제 없음
- 장기간의 집계에도 시간이 오래 걸리지 않음
- 매일 데이터 마트를 다시 만들어 결과를 치환하면 멱등이 된다.

# 워크플로 관리 도구에 의한 자동화
## Airflow
- 워크플로는 여러 태스크로 이루어진 DAG의 형태로 정의
- 의존 관계가 없는 태스크는 병렬로 실행
- 모든 태스크를 시간과 관련지어 보존
## 워크플로를 터미널로부터 실행하기
- 파이썬 스크립트를 실행하기 위해서 aifrflow 명령어를 호출해야 함
- airflow test: 개별 태스크 테스트
- airflow backfill: DAG에 포함되는 모든 태스크 실행
## 스케줄러를 기동하여 DAG를 정기 실행하기
- airflow scheduler: 스케줄러 기동
- 모든 DAG는 실행 간격과 시작 시간을 지정해야 한다.
- DAG는 스케줄 된 간격이 끝날 때 실행해야 한다. (1/1 태스크는 1/2 되는 순간에 실행)
## 태스크가 소비하는 자원 제어하기
- 태스크 실행 시간이 길어지지 않도록 하기
- 자원 풀 구조: 태스크의 동시 실행을 제한
- Airflow 워커를 원격으로 실행할 수 있다
