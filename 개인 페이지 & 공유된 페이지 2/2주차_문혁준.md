# 2주차

### 3-1 대규모 분산 처리의 프레임워크

- 구조화된 데이터
    - 스키마가 명확하게 정의된 데이터
- 비구조화된 데이터
    - 스키마가 없는 데이터
    - CSV, JSON, XML 등의 데이터
    - 서식은 정해져 있지만, 칼럼 수나 데이터형은 명확하지 않은 데이터
- 데이터 구조화의 파이프라인
    - 스키마로 명확하게 한 테이블 형식의 구조화 데이터로 변환
- Hadoop
    - 분산 시스템을 구성하는 다수의 소프트웨어로 이루어진 집합체
    - 분산 파일 시스템: HDFS
    - 리소스 관리자: YARN
    - 분산 데이터 처리: MapReduce
- YARN 컨테이너
    - 애플리케이션마다 실행의 우선순위를 정할 수 있음
    - 한정된 리소스를 낭비 없이 활용하며 데이터 처리가 가능
- MapReduce
    - 대량의 데이터를 배치 처리하기 위한 시스템

- 대화형 퀴리 엔진
    - Hadoop에서는 대량의 비구조화 데이터를 가공하는 배치처리 이용
    - 완성된 구조화된 데이터를 대화식으로 집계하고자 할때, Impala와 Presto 사용

- Spark
    - 대량의 메모리를 활용하여 고속화 실현
    - 가능한 한 많은 데이터를 메모리 상에 올린 상태로 두어 디스크 I/O 없앰
    - 비정상 종료되면, 중간 데이터는 삭제되지만, 다시 만들면 된다는 개념
    - MapReduce를 대체하는 존재
    - 자바나 파이썬 같은 스크립트 언어 사용 가능
    - 분산 스토리지로 Amazon S3, 카산드라 사용 가능

### 3-2 쿼리 엔진

- 데이터 마트 구축의 파이프라인
1. Hive에 의한 구조화 데이터 작성
2. 열 지향 스토리지로 변환

- Hive로 비정규화 테이블 작성
    - 서브 쿼리 안에서 레코드 수를 줄이는 방법
    - 데이터의 편향을 방지하는 방법

- 대화형 쿼리 엔진 Presto 구조
    - Hive에서 만든 구조화된 데이터를 좀 더 집계하는 목적에 적합
    - 열 지향 스토리지로 되어 있어야 성능 최대

- 데이터 분석의 프레임 워크 선택
    - MPP 데이터베이스
        - 완성된 비정규화 테이블의 고속 집계에 적합
    - Hive
        - 높은 확장성과 내결함성 목표 설계
        - 대규모 배치 처리를 꾸준히 실행
        - 텍스트 데이터 가공 혹은 열 지향 스토리지 만드는 등의 처리에 적합
    - Presto
        - 대화형 쿼리 엔진
        - 실행 중 장애가 발생하면 오류가 떠서 처음부터 다시 실행
        - 오류가 발생하면 다시 반복해서 실행
        - 모든 데이터를 SQL로 집계하기 위한 중심적인 존재
        - 텍스트 처리가 중심이 되는 ETL 프로세스 및 데이터 구조화에는 부적합
    - Spark
        - 대화형 쿼리 실행에 적합
        - ETL 프로세스에서 SQL에 이르기까지의 일련의 흐름을 하나의 데이터 파이프라인으로 기술 가능
        - 분산 시스템을 활용한 환경 → ETL 프로세스나 머신러닝 같은 모든 데이터 처리에 사용 가능

### 3-3 데이터 마트의 구축

- 팩트 테이블
    - 추가: 새로 도착한 데이터만을 증분으로 추가
    - 치환: 과거의 데이터를 포함하여 테이블 전체 치환
- 테이블 파티셔닝
    - 하나의 테이블을 여러 물리적인 파티션으로 나눔으로써 파티션 단위로 정리하여 데이터를 쓰거나 삭제할 수 있게 한 것

- 집계 테이블
    - 팩트 테이블을 어느 정도 모아서 집계한 것
- 스냅샷 테이블
    - 정기적으로 테이블을 통째로 저장한 것
- 이력 테이블
    - 모든 데이터가 아닌, 변경된 데이터만을 증분으로 스냅샷
    - 변경이 있을 때마다 그 내용을 기록

### 4-1 벌크 형과 스트리밍 형의 데이터 수집

- 벌크 형의 데이터 전송
    - 미리 저장된 데이터나 수집한 데이터를 ETL 서버를 통해 스토리지에 저장
    - ETL 서버는 구조화된 데이터 처리에 적합한 프로세스로 구성
    - 많은 양의 데이터를 전송하므로, 분산 스토리지에 저장

- 스트리밍 형의 데이터 전송
    - 지금 바로 생성되어 아직 어디에도 저장되지 않은 데이터
    - 계속해서 작은 데이터가 전송(메시지 배송)
    - NoSQL DB 사용(작은 데이터 쓰기에 적합한 쿼리 엔진)
- 웹 브라우저에서 메시지 배송
    - 서버상에서 일단 데이터를 축적해놓고 나중에 모아서 배송
    - Fluentd, Logstash와 같은 상주형 로그 수집 소프트웨어
- 모바일 앱으로부터의 메시지 배송
    - 웹 브라우저에서의 전송 방식과 동일
    - 추가로 백 엔드의 각종 서비스를 이용하여 배송 가능
- 디바이스로부터의 메시지 배송
    - MQTT

### 4-2 메시지 배송의 트레이드 오프

- 메시지 브로커
    - 대량의 메시지를 안정적으로 받기 위해 설치
    - 데이터를 일시적으로 축적하는 중산층의 기능
    - 분산 스토리지의 성능적 한계를 보완하기 위해 설치
    - Apache Kafka, Amazone Kinesis

- 푸쉬형
    - 송신 측에 제어로 데이터를 보내는 방식
- 풀 형
    - 수신 측의 주도로 데이터를 가져오는 것
- 스트림 처리
    - 메시지 브로커에 데이터를 푸쉬한 후, 소비자(데이터 꺼내오는 측)에서 가져옴
    - 이 과정을 1초마다 반복하여 실시간으로 처리
- 메시지 라우팅
    - 메시지 브로커에 써넣은 데이터를 복수의 다른 소비자에서 읽어들임
    - 메시지가 복사되어 데이터를 여러 경로로 분기

- 메시지 배송의 어려운 점
    - 신뢰성 문제 → 반드시 메시지의 중복이나 누락이 발생
- at most once 설계
    - 메시지는 한 번만 전송 → 누락이 되는 문제 발생
- exactly once 설계
    - 메세지는 손실되거나 중복 없이 한번 만 전달
    - 코디네이터(중계자)의 부재의 경우 문제 발생
- at least once
    - 중복 제거 → 사용자에게 맡김
- 중복 제거 방식
    - 오프셋을 이용한 중복 제거
    - 고유 ID에 의한 중복 제거
    - 종단간의 신뢰성
- 고유 ID를 사용한 중복 제거 방법
    - 분산 스토리지로 NoSQL DB 사용
    - SQL로 중복을 제거

### 4-3 시계열 데이터의 최적화

- 풀 스캔
    - 다수의 파일을 모두 검색하는 쿼리
    - 시스템의 부하를 크게 높이는 요인
    - 가능한 피해야 하는 부분
- 시계열 인덱스
    - 이벤트 시간 취급을 효율화 하기 위해 데이터를 정렬하는 것
    - 매우 짧은 범위의 특정 시간에 맞춘 데이터 집계를 빠르게
    - 단기적인 데이터 분석에 유리 (장기적에는 불리)
- 조건절 푸쉬다운
    - 매일 한 번씩 새로 도착한 데이터를 배치 처리로 변환
    - 칼럼 단위의 통계 정보 이용 → 필요 최소한의 데이터만 읽음
- 이벤트 시간에 의한 분할
    - 시계열 테이블: 시간을 이용하여 분할한 테이블
    - 이벤트 시간으로 분할되어 데이터 검색에 더 효율적

### 4-4 비구조화 데이터의 분산 스토리지

- NoSQL 데이터베이스에 의한 데이터 활용
    - 객체 스토리지 상의 파일은 교체하기 어려움
    - 특정 용도에 최적화된 데이터 저장소가 필요
- 분산 KVS
    - 모든 데이터를 키값 쌍으로 저장하도록 설계된 데이터 저장소
    - 노드 간에 부하를 균등하게 분산, 노드를 증감하는 것만으로 클러스터 성능 변경 가능
    - Amazone Dynamo DB
- 와이드 칼럼 스토어
    - 분산 KVS를 발전시켜 2개 이상의 임의의 키에 데이터를 저장할 수 있도록 한 것
    - 새로운 행뿐만 아니라, 칼럼도 얼마든지 추가 가능
    - 하나의 테이블에 가로와 세로의 2차원에 데이터를 쓸 수 있도록 한 것
    - Apache Cassandra
- 도규먼트 스토어
    - 데이터 처리의 유연성
    - 배열과 연상 배열과 같은 중첩된 데이터 구조에 대해 인덱스를 만들거나, 도큐먼트 일부만을 치환하는 식의 쿼리를 쉽게 실행 가능
    - 스키마를 정하지 않고 데이터 처리가 가능
    - Mongo DB
- 검색 엔진
    - 텍스트 데이터를 전문 검색하기 위해 역 색인을 만듦
    - Elasticssearch
    - Splunk